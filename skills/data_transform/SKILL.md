---
name: data_transform
description: TasarÄ±m ve verimlilik odaklÄ± veri dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (ETL/ELT) sÃ¼reÃ§leri, dbt ve Polars optimizasyonu.
metadata:
  skillport:
    category: data
    tags: [architecture, automation, best practices, clean code, coding, collaboration, compliance, data transform, database migration, debugging, design patterns, development, documentation, efficiency, git, optimization, productivity, programming, project management, quality assurance, refactoring, software engineering, standards, testing, utilities, version control, workflow]      - etl
---

# ğŸ”„ Data Transformation (ETL/ELT)

> Veri dÃ¶nÃ¼ÅŸÃ¼mÃ¼, boru hatlarÄ± ve optimizasyon rehberi.

---

## ğŸ—ï¸ Transformation Architectures

### ETL (Extract, Transform, Load)
- **Use Case**: Traditional data warehousing.
- **Tools**: Python (Pandas/Polars), Spark.
- **Benefit**: Cleaner data in early stages.

### ELT (Extract, Load, Transform)
- **Use Case**: Modern cloud data warehouses (Snowflake, BigQuery).
- **Tools**: dbt, SQL.
- **Benefit**: Scalability and flexibility.

---

## ğŸ“Š Pipeline Checklist

```markdown
- [ ] Schema validation at source
- [ ] Idempotent transformations
- [ ] Error handling & dead-letter queues
- [ ] Data quality tests (dbt tests)
- [ ] Documentation of lineage
```

## Detailed Topics

### Incremental Processing

```python
def extract_incremental(last_run_date):
    query = f"""
        SELECT * FROM source_table
        WHERE updated_at > '{last_run_date}'
    """
    return pd.read_sql(query, conn)
```

### Error Handling

```python
def safe_transform(data):
    try:
        transformed = transform_data(data)
        return transformed
    except Exception as e:
        logger.error(f"Transform failed: {e}")
        send_alert(f"Pipeline failed: {e}")
        raise
```

## ğŸ”„ Workflow

> **Kaynak:** [dbt Labs - Best Practices](https://docs.getdbt.com/best-practices) & [Polars Performance Guide](https://pola-rs.github.io/polars-book/user-guide/optimizations/lazy/)

### AÅŸama 1: Data Contract & Source Audit
- [ ] **Data Contracts**: Veri kaynaÄŸÄ± (Source) ve hedef (Target) arasÄ±ndaki ÅŸemayÄ± sabitle.
- [ ] **Profiling**: Ham verideki eksikleri, null oranlarÄ±nÄ± ve tipleri (Profiling) analiz et.
- [ ] **Pattern Selection**: Veri boyutuna gÃ¶re ETL (Pandas/Polars) veya ELT (SQL/dbt) seÃ§imi yap.

### AÅŸama 2: Transformation Engine Setup
- [ ] **Infrastructure**: `dbt-core` profilini kur veya Cloud IDE yapÄ±landÄ±r.
- [ ] **Modular Modeling**: Veriyi Staging (Renaming), Intermediate (Logic) ve Marts (Final) katmanlarÄ±na ayÄ±r.
- [ ] **Polars Optimization**: Python tabanlÄ± dÃ¶nÃ¼ÅŸÃ¼mlerde `lazy` modunu (`scan_csv` / `collect`) kullanarak bellek ve hÄ±z optimizasyonu yap.

### AÅŸama 3: Testing & Orchestration
- [ ] **Unit Tests**: Kritik dÃ¶nÃ¼ÅŸÃ¼m mantÄ±ÄŸÄ± iÃ§in `dbt tests` veya `Great Expectations` ile validation yaz.
- [ ] **Idempotency**: Boru hattÄ±nÄ±n (Pipeline) hata durumunda tekrar Ã§alÄ±ÅŸtÄ±rÄ±labilir (Idempotent) olduÄŸundan emin ol.
- [ ] **Orchestration**: Ä°ÅŸ akÄ±ÅŸÄ±nÄ± Airflow veya Dagster Ã¼zerinde takvime baÄŸla ve hata bildirimlerini kur.

### Kontrol NoktalarÄ±
| AÅŸama | DoÄŸrulama |
|-------|-----------|
| 1 | DÃ¶nÃ¼ÅŸÃ¼m sonrasÄ± veri kaybÄ± yaÅŸandÄ± mÄ±? (Check Sum) |
| 2 | dbt modellerinde `ref` fonksiyonu dÄ±ÅŸÄ±nda hardcoded tablo ismi kullanÄ±ldÄ± mÄ±? |
| 3 | Pipeline baÅŸarÄ±sÄ±z olduÄŸunda "Rollback" veya "Reprocessing" stratejisi var mÄ±? |

---
*Data Transformation v2.0 - With Workflow*
