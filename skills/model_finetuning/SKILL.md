---
name: model_finetuning
router_kit: AIKit
description: LLM fine-tuning techniques (LoRA, QLoRA), dataset preparation ve alignment strategies (DPO, RLHF).
metadata:
  skillport:
    category: ai
    tags: [alignment, artificial intelligence, automation, dataset preparation, deep learning, dpo, fine-tuning, generative ai, huggingface, large language models, llm, lora, machine learning, model finetuning, natural language processing, neural networks, nlp, optimization, performance, prompt engineering, qlora, rlhf, sft, training, workflow]      - huggingface-transformers
---

# ğŸ§  Model Fine-Tuning

> BÃ¼yÃ¼k dil modellerine (LLM) yeni bilgiler Ã¶ÄŸretme veya davranÄ±ÅŸlarÄ±nÄ± Ã¶zelleÅŸtirme.

---

## ğŸš€ Fine-Tuning Stages

1. **SFT (Supervised Fine-Tuning)**: Modelin "Soru-Cevap" veya "Talimat" formatÄ±nÄ± Ã¶ÄŸrenmesi.
2. **Alignment (Hizalama)**: Modelin insan tercihlerine gÃ¶re eÄŸitilmesi.
   - **DPO (Direct Preference Optimization)**: Basit ve verimli tercih optimizasyonu (Ã–nerilen).
   - **RLHF (Reinforcement Learning from Human Feedback)**: KarmaÅŸÄ±k Ã¶dÃ¼l mekanizmalarÄ±.

---

## ğŸ› ï¸ Parameter Efficient Fine-Tuning (PEFT)

TÃ¼m modeli eÄŸitmek yerine sadece kÃ¼Ã§Ã¼k bir kÄ±smÄ±nÄ± eÄŸiterek kaynak tasarrufu saÄŸlayan yÃ¶ntemler.

| Teknik | AÃ§Ä±klama |
|--------|----------|
| **LoRA** | AÄŸÄ±rlÄ±k matrislerine dÃ¼ÅŸÃ¼k rÃ¼tbeli (Low-Rank) katmanlar ekler. |
| **QLoRA** | LoRA'yÄ± 4-bit quantization ile birleÅŸtirir (Minimum RAM). |

---

## ğŸ“Š Dataset Preparation

Veri seti kalitesi, veri miktarÄ±ndan daha Ã¶nemlidir.
- **Format**: `{"instruction": "...", "input": "...", "output": "..."}`
- **Diversity**: FarklÄ± senaryolarÄ± kapsayan Ã§eÅŸitlilik.
- **Cleaning**: HatalÄ±, tekrarlÄ± ve dÃ¼ÅŸÃ¼k kaliteli verilerin temizlenmesi.

---

*Model Finetuning v1.1 - Enhanced*

## ğŸ”„ Workflow

> **Kaynak:** [HuggingFace TRL Docs](https://huggingface.co/docs/trl/index) & [DeepLearning.ai LLM Engineering](https://www.deeplearning.ai/)

### AÅŸama 1: Preparation (Data-Centric AI)
- [ ] **Data Cleaning**: Veri setini deduplicate et ve kalite kontrolÃ¼ yap (PII temizliÄŸi).
- [ ] **Format**: Dataset'i modele uygun formata (ShareGPT, Alpaca vb.) dÃ¶nÃ¼ÅŸtÃ¼r.
- [ ] **Baseline**: Base modelin performansÄ±nÄ± (Zero-shot) Ã¶lÃ§ ve kaydet.

### AÅŸama 2: Training (Parameter Efficient)
- [ ] **LoRA/QLoRA**: Full fine-tuning yerine LoRA (Rank 16-64) kullan (Daha az VRAM, %95+ performans).
- [ ] **Monitoring**: WandB veya MLflow ile Loss eÄŸrilerini ve eval metriklerini canlÄ± izle.
- [ ] **Checkpointing**: Her epoch veya belirli stepte model aÄŸÄ±rlÄ±klarÄ±nÄ± kaydet.

### AÅŸama 3: Alignment & Evaluation
- [ ] **Alignment**: SFT sonrasÄ± gerekiyorsa DPO veya PPO ile insan tercihlerine hizala.
- [ ] **Evaluation**: `llm_evaluation` skill'ini kullanarak otomatik ve manuel testler yap.
- [ ] **Merging**: LoRA adaptÃ¶rlerini base modele merge et ve quantize (GGUF/AWQ) yap.

### Kontrol NoktalarÄ±
| AÅŸama | DoÄŸrulama |
|-------|-----------|
| 1 | Training Loss dÃ¼ÅŸerken Validation Loss artÄ±yor mu (Overfitting)? |
| 2 | Model "Catastrophic Forgetting" yaÅŸÄ±yor mu (Eski yeteneklerini kaybetti mi)? |
| 3 | Inference hÄ±zÄ± ve belleÄŸi (VRAM) deployment ortamÄ±na uygun mu? |
