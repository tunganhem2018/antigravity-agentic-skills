---
name: llm_evaluation
router_kit: AIKit
description: LLM Ã§Ä±ktÄ± kalitesini Ã¶lÃ§me, benchmarking, RAG evaluation ve model karÅŸÄ±laÅŸtÄ±rma teknikleri.
metadata:
  skillport:
    category: ai
    tags: [ai metrics, ai monitoring, benchmarks, bias detection, bleue, evaluation, hallucination, llm evaluation, model alignment, model performance, nlp evaluation, quality assurance, rag evaluation, reliability, rouge, safety, testing, validation]      - prompt-engineering
---

# ğŸ“Š LLM Evaluation

> LLM Ã§Ä±ktÄ±larÄ±nÄ±n kalitesini, doÄŸruluÄŸunu ve gÃ¼venilirliÄŸini Ã¶lÃ§me.

---

## ğŸ“ Evaluation Metrics

### 1. NLP Metrics (Automated)
- **ROUGE/BLEU**: Metin benzerliÄŸi (Ã–zetleme/Ã‡eviri iÃ§in).
- **Exact Match (EM)**: Soru-cevap testlerinde doÄŸrudan eÅŸleÅŸme.

### 2. Model-based Metrics
- **LLM-as-a-Judge**: GÃ¼Ã§lÃ¼ bir modeli (GPT-4), diÄŸer modelin Ã§Ä±ktÄ±sÄ±nÄ± 1-10 arasÄ± puanlamak iÃ§in kullanma.
- **RAGAS**: RAG sistemleri iÃ§in (faithfulness, answer relevance).

### 3. Human Evaluation
- **Elo Rating**: Ä°ki modelin Ã§Ä±ktÄ±sÄ±nÄ± kÃ¶rleme test (Blind test) ile karÅŸÄ±laÅŸtÄ±rma.

---

## ğŸ—ï¸ Evaluation Categories

| Kategori | Ne Ã–lÃ§Ã¼lÃ¼r? |
|----------|-------------|
| **Accuracy** | Ãœretilen bilgi doÄŸru mu? |
| **Hallucination** | Model uyduruyor mu? |
| **Safety/Bias** | ZararlÄ± veya yanlÄ± iÃ§erik var mÄ±? |
| **Format** | Ã‡Ä±ktÄ± istenen formatta (JSON/XML) mÄ±? |
| **Latency** | YanÄ±t hÄ±zÄ± Ã¼retim iÃ§in uygun mu? |

---

*LLM Evaluation v1.1 - Enhanced*

## ğŸ”„ Workflow

> **Kaynak:** [Ragas Documentation](https://docs.ragas.io/en/stable/) & [OpenAI Evals](https://github.com/openai/evals)

### AÅŸama 1: Benchmark Dataset (Gold Set)
- [ ] **Samples**: El ile doÄŸrulanmÄ±ÅŸ 50-100 adetlik "soru-cevap" veri seti oluÅŸtur.
- [ ] **Edge Cases**: Modelin zorlanabileceÄŸi "trick questions" veya boÅŸ yanÄ±t gerektiren durumlarÄ± ekle.

### AÅŸama 2: RAG Evaluation
- [ ] **Faithfulness**: YanÄ±t, saÄŸlanan context'e dayanÄ±yor mu?
- [ ] **Answer Relevance**: YanÄ±t soruyla ne kadar alakalÄ±?
- [ ] **Context Precision**: SaÄŸlanan context iÃ§inde doÄŸru bilgi ne kadar yukarda?

### AÅŸama 3: Automated Pipeline
- [ ] **CI/CD Integration**: Her prompt deÄŸiÅŸikliÄŸinde eval setini otomatik Ã§alÄ±ÅŸtÄ±r.
- [ ] **Thresholds**: Metrikler belirlenen sÄ±nÄ±rÄ±n altÄ±na dÃ¼ÅŸerse (Ã¶rn: Accuracy < %80) uyarÄ± ver.

### Kontrol NoktalarÄ±
| AÅŸama | DoÄŸrulama |
|-------|-----------|
| 1 | Puanlayan model (Judge) ile test edilen model aynÄ± mÄ±? (YapÄ±lmamalÄ±!) |
| 2 | Eval seti gerÃ§ek kullanÄ±cÄ± verilerini temsil ediyor mu? |
| 3 | HalÃ¼sinasyon oranÄ± kritik seviyenin altÄ±nda mÄ±? |
